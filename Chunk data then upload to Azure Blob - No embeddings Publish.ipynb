{"cells":[{"cell_type":"markdown","source":["**This script will take a source directory in Azure blob storage and then download and chunk up the documents in it based on the chunking parameters you provide. It will then upload a resulting JSON file for each chunk that contains the chunked content back to a destination directory in Azure Blob storage. It will also create an Azure Cognitive Search index, indexer, and data source you can use to easily pull in the data to Azure Cognitive Search. Once the entire script has run just run the indexer in Azure Cognitive search and it will automatically pull in the chunked JSON files. Make sure to have Azure Cognitive Search Semantic Search turned on.**\r\n","\r\n","**The Resulting JSON files and Azure Search Index can easily be used with the 'Azure OpenAI on your Data' service.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"be3fdbec-b29d-4c66-8f19-cf99d6d7e7ff"},{"cell_type":"code","source":["pip install pypdf azure-ai-formrecognizer azure-identity langchain azure-storage-blob python-dotenv unstructured openai azure-search-documents"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"719835d5-a617-4618-9553-62af3b1c1df9"},{"cell_type":"code","source":["pip install azure-storage-file-datalake --pre"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a4d7e219-1762-49ce-83af-f45d96b1fa5d"},{"cell_type":"markdown","source":["**Update the fields below with your Azure services information and keys:**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"efc179c0-b6f9-4973-91df-cd91df2612f6"},{"cell_type":"code","source":["import os, uuid\r\n","from azure.identity import DefaultAzureCredential\r\n","from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n","from azure.core.credentials import AzureKeyCredential\r\n","from azure.ai.formrecognizer import DocumentAnalysisClient\r\n","from azure.storage.filedatalake import DataLakeServiceClient\r\n","from azure.identity import DefaultAzureCredential\r\n","from azure.identity import ClientSecretCredential\r\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n","import openai\r\n","import json\r\n","import requests\r\n","import random\r\n","import string\r\n","import time\r\n","\r\n","#OpenAI connectivity \r\n","openai.api_type = \"azure\"\r\n","openai.api_key = \"\"\r\n","openai.api_base = \"https://<your openai instance>.openai.azure.com/\"\r\n","openai.api_version = \"2022-12-01\"\r\n","\r\n","\r\n","# This writes a file to Azure Blob Store - replace with your connection string\r\n","os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"] = \"DefaultEndpointsProtocol=https;AccountName=dhadls;AccountKey=\" # Replace with your connection string\r\n","\r\n","# environment variable into account.\r\n","connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\r\n","dfs_account_url = \"https://<your storage account>.dfs.core.windows.net\" #Use DFS url here\r\n","blob_account_url = \"https://<your storage account>.blob.core.windows.net\"\r\n","source_container_name = \"\"\r\n","source_path_name = \"/\"\r\n","\r\n","# Create a unique name for the container for the chunked documents\r\n","destination_container_name = \"\"\r\n","\r\n","\r\n","#Name of search index, indexer, and data source you want to create in Azure Cog Search\r\n","index_name = \"\"\r\n","indexer_name = \"\"\r\n","data_source_name = \"\"\r\n","\r\n","#Cognitive Search Connection\r\n","search_endpoint = \"https://<your cognitive search>.search.windows.net\"\r\n","search_endpoint_for_creating_index = \"https://<your cognitive search>.search.windows.net/indexes?api-version=2023-07-01-Preview\"\r\n","search_endpoint_for_creating_indexer = \"https://<your cognitive search>.search.windows.net/indexers?api-version=2023-07-01-Preview\"\r\n","search_endpoint_for_creating_datasource = \"https://<your cognitive search>.search.windows.net/datasources?api-version=2023-07-01-Preview\"\r\n","search_api_key =\"\" #Cog search admin key\r\n","\r\n","#Forms recognizer details\r\n","forms_recognizer_url = \"https://<your form recognizer>.cognitiveservices.azure.com\"\r\n","forms_recognizer_preview_url = \"https://<your form recognizer.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-read:analyze?api-version=2022-06-30-preview\"\r\n","forms_recognizer_key = \"\"\r\n","\r\n","#Chunk parameters\r\n","chunk_size = 1000\r\n","chunk_overlap = 200\r\n","\r\n","\r\n","\r\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d86dc257-5f53-464d-ad2f-a48a9c1be68d"},{"cell_type":"markdown","source":["**This section will create the index in Azure Cog Search that will store the data after the indexer is run. No need to update anything in these fields.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3361c539-600e-4561-9f72-5923b8d7a55a"},{"cell_type":"code","source":["## This creates the JSON that will create the search index with vector search enabled.\r\n","\r\n","search_json = {\r\n","    \"name\": f\"{index_name}\",\r\n","    \"fields\": [\r\n","        {\r\n","            \"name\": \"key\",\r\n","            \"type\": \"Edm.String\",\r\n","            \"searchable\": False,\r\n","            \"retrievable\": True,\r\n","            \"key\": True,\r\n","            \"filterable\": False,\r\n","            \"facetable\": False,\r\n","            \"sortable\": False\r\n","        },\r\n","        {\r\n","            \"name\": \"title\",\r\n","            \"type\": \"Edm.String\",\r\n","            \"searchable\": True,\r\n","            \"retrievable\": True,\r\n","            \"key\": False,\r\n","            \"filterable\": False,\r\n","            \"facetable\": False,\r\n","            \"sortable\": False\r\n","        },\r\n","        {\r\n","            \"name\": \"content\",\r\n","            \"type\": \"Edm.String\",\r\n","            \"searchable\": True,\r\n","            \"retrievable\": True,\r\n","            \"key\": False,\r\n","            \"filterable\": False,\r\n","            \"facetable\": False,\r\n","            \"sortable\": False\r\n","        },\r\n","        {\r\n","            \"name\": \"url\",\r\n","            \"type\": \"Edm.String\",\r\n","            \"searchable\": True,\r\n","            \"retrievable\": True,\r\n","            \"key\": False,\r\n","            \"filterable\": False,\r\n","            \"facetable\": False,\r\n","            \"sortable\": False\r\n","        },\r\n","        {\r\n","            \"name\": \"filename\",\r\n","            \"type\": \"Edm.String\",\r\n","            \"searchable\": True,\r\n","            \"retrievable\": True,\r\n","            \"key\": False,\r\n","            \"filterable\": False,\r\n","            \"facetable\": False,\r\n","            \"sortable\": False\r\n","        }\r\n","    ],\r\n","    \"corsOptions\": {\r\n","        \"allowedOrigins\": [\r\n","            \"*\"\r\n","        ],\r\n","        \"maxAgeInSeconds\": 60\r\n","    },\r\n","    \"vectorSearch\": {\r\n","        \"algorithmConfigurations\": [\r\n","            {\r\n","                \"name\": \"my-vector-config\",\r\n","                \"kind\": \"hnsw\",\r\n","                \"hnswParameters\": {\r\n","                    \"m\": 4,\r\n","                    \"efConstruction\": 400,\r\n","                    \"metric\": \"cosine\"\r\n","                }\r\n","            }\r\n","        ]\r\n","    },\r\n","    \"semantic\": {\r\n","        \"configurations\": [\r\n","            {\r\n","                \"name\": \"my-semantic-config\",\r\n","                \"prioritizedFields\": {\r\n","                    \"titleField\": {\r\n","                        \"fieldName\": \"title\"\r\n","                    },\r\n","                    \"prioritizedContentFields\": [\r\n","                        {\r\n","                            \"fieldName\": \"content\"\r\n","                        }\r\n","                    ],\r\n","                    \"prioritizedKeywordsFields\": [\r\n","                        {\r\n","                            \"fieldName\": \"content\"\r\n","                        }\r\n","                    ]\r\n","                }\r\n","            }\r\n","        ]\r\n","    }\r\n","}\r\n","\r\n","#This section makes the REST call to create the index\r\n","\r\n","url = f'{search_endpoint_for_creating_index }'\r\n","print(\"URL is \" + url)\r\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\r\n","print(\"Search api key is\" +search_api_key)\r\n","print(\"Search JSON is \" + str(search_json))\r\n","print(\"About to make the rest call\")\r\n","#print(\"data is\" + str(search_json))\r\n","response = requests.post(search_endpoint_for_creating_index , headers=headers, data=json.dumps(search_json))\r\n","\r\n","if response.status_code == 200 or response.status_code == 201:\r\n","    print(\"Index created successfully\")\r\n","else:\r\n","    print('Error:', response.status_code)       \r\n","\r\n","print(\"rest call completed\")   "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b79fece1-c597-41cd-914f-b68a84a9f497"},{"cell_type":"markdown","source":["**This section will create the data source in Azure Cog Search that the indexer will use to load the data. It references the location of the chunked data above. No need to update anything in this section**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3aebb233-892c-49a4-8b2a-1d1c70c75c99"},{"cell_type":"code","source":["data_source_request_json = {   \r\n","    \"name\" : data_source_name,    \r\n","    \"type\" : \"azureblob\",\r\n","    \"credentials\" : { \"connectionString\" :connect_str },\r\n","    \"container\": {\r\n","        \"name\": destination_container_name\r\n","    }\r\n","}  \r\n","\r\n","url = f'{search_endpoint_for_creating_index }'\r\n","print(\"URL is \" + url)\r\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\r\n","#print(\"Search api key is\" +search_api_key)\r\n","print(\"About to make the REST call\")\r\n","print(\"Request JSON is \" +json.dumps(data_source_request_json))\r\n","#print(\"data is\" + str(search_json))\r\n","response = requests.post(search_endpoint_for_creating_datasource , headers=headers, data=json.dumps(data_source_request_json))\r\n","\r\n","data = response.json()\r\n","print(data)\r\n","\r\n","if response.status_code == 200 or response.status_code == 201:\r\n","    print(\"Create data source call completed successfully\") \r\n","else:\r\n","    print('Error:', response.status_code)       \r\n","\r\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fdf21b54-f126-4e8d-b390-f40c6fa88225"},{"cell_type":"markdown","source":["**This section will create the Azure Cognitive Search indexer. No need to update anything in these fields.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2142aa12-4a84-4a93-9dcd-ffbe3f1365cc"},{"cell_type":"code","source":["# This code creates an indexer that will load the data to the index above\r\n","request_json = {\r\n","    \"name\": indexer_name,\r\n","    \"dataSourceName\": data_source_name,\r\n","    \"targetIndexName\": index_name,\r\n","    \"parameters\": {\r\n","        \"configuration\": {\r\n","        \"allowSkillsetToReadFileData\": False,\r\n","        \"parsingMode\": \"json\"\r\n","        }\r\n","    },\r\n","    \"fieldMappings\": [],\r\n","    \"outputFieldMappings\": [],\r\n","    }\r\n","\r\n","\r\n","url = f'{search_endpoint_for_creating_index }'\r\n","print(\"URL is \" + url)\r\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\r\n","print(\"Search api key is\" +search_api_key)\r\n","print(\"About to make the rest call\")\r\n","print(\"request json is \" +json.dumps(request_json))\r\n","#print(\"data is\" + str(search_json))\r\n","response = requests.post(search_endpoint_for_creating_indexer , headers=headers, data=json.dumps(request_json))\r\n","\r\n","data = response.json()\r\n","print(data)\r\n","\r\n","if response.status_code == 200 or response.status_code == 201:\r\n","    print(\"Create indexer call completed successfully\") \r\n","else:\r\n","    print('Error:', response.status_code)       \r\n","\r\n","print(\"Indexer call completed\") "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"494e6a36-ebe6-4f75-8677-beba67bcccf7"},{"cell_type":"markdown","source":["**This section below will read the data from the source storage account, chunk it, then upload a JSON file that can be ingested into Azure Cognitive Search when the indexer is run. No need to add or update anything to this section to run it. **"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b35dbb50-aff0-4e79-89fd-5ebbb4fe5fce"},{"cell_type":"code","source":["\r\n","# This function below is for loading the chunks back to Azure storage, called at the end of this section\r\n","def upload_blob_data(blob_service_client: BlobServiceClient, destination_container_name: str, blob_name):\r\n","    blob_client = blob_service_client.get_blob_client(container=destination_container_name, blob=blob_name)\r\n","    data = json_string\r\n","    # Upload the blob data - default blob type is BlockBlob\r\n","    blob_client.upload_blob(data, blob_type=\"BlockBlob\")\r\n","\r\n","\r\n","def list_blobs_flat(blob_service_client: BlobServiceClient, source_container_name):\r\n","    container_client = blob_service_client.get_container_client(container=source_container_name)\r\n","    blob_list = container_client.list_blobs()\r\n","    #for blob in blob_list:\r\n","    #    print(f\"{blob.name}\")\r\n","    return blob_list\r\n","\r\n","#This function generates a random string we can use for the Azure Search key field\r\n","def generate_random_string(length):\r\n","    characters = string.ascii_letters + string.digits\r\n","    random_string = ''.join(random.choice(characters) for _ in range(length))\r\n","    return random_string\r\n","\r\n","blob_service_client = BlobServiceClient.from_connection_string(connect_str)\r\n","blob_listing = list_blobs_flat(blob_service_client, source_container_name)\r\n","#The section below will loop through each of the documents paths, parse the files using forms recognizer, generate the embeddings, then upload each chunk to Azure storage\r\n","count = 0\r\n","for blob in blob_listing:\r\n","    print(blob.name + '\\n')\r\n","\r\n","    ##This section uses Forms Recognizer to read a file in blob store. We will use a REST call since we are using the preview API service which supports PDF, Doc, HTML and more. The Python client doesn't support the preview API yet.\r\n","    endpoint = forms_recognizer_url\r\n","    key = forms_recognizer_key\r\n","    formUrl =  f\"{blob_account_url}/{source_container_name}/{blob.name}\"\r\n","    print(formUrl)\r\n","    request_json = {\r\n","        \"urlSource\": f\"{formUrl}\",\r\n","    }\r\n","    \r\n","    headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': forms_recognizer_key}\r\n","    response = requests.post(forms_recognizer_preview_url , headers=headers, data=json.dumps(request_json))\r\n","    print (response)\r\n","    response_url = response.headers.get('Operation-Location')\r\n","    print(response_url)\r\n","    # It takes a few seconds for the Forms recognizer to parse the document, so this sleeps for 20 seconds\r\n","    time.sleep(20) \r\n","    # Get the parsed document response from the Forms Recognizer service\r\n","    response2 = requests.get(response_url , headers=headers)\r\n","    data = response2.json()\r\n","    content = data['analyzeResult']['content']\r\n","    #print(\"Content is \" + str(content))\r\n","\r\n","    #This will chunk up the document\r\n","    text_splitter = RecursiveCharacterTextSplitter(\r\n","        chunk_size = chunk_size,\r\n","        chunk_overlap  = chunk_overlap,\r\n","        length_function = len,\r\n","    )\r\n","\r\n","    texts = text_splitter.create_documents([content])\r\n","\r\n","    #print(texts[0])\r\n","\r\n","    x = len(texts)\r\n","    #print(x)\r\n","\r\n","    #This takes the url of the document and stores it with no spaces\r\n","    formUrl = formUrl.replace(\" \", \"%20\")\r\n","    print(\"Now writing each chunk to blob storage..\")\r\n","    for item in texts:\r\n","        count = count + 1\r\n","        ##print(item)\r\n","        item = str(item)\r\n","\r\n","        # Generate a random string of length 15 for the Azure Search key\r\n","        random_str = generate_random_string(15)\r\n","\r\n","        #item = str(item) //use this for just straight chunking without using a json format\r\n","        json_data = {\r\n","            \"key\": f\"{random_str}\",\r\n","            \"title\": f\"{blob.name}\",\r\n","            \"content\": f\"{item}\",\r\n","            \"url\": f\"{formUrl}\",\r\n","            \"filename\": f\"{blob.name}\"\r\n","        }\r\n","\r\n","        json_string = json.dumps(json_data)\r\n","        #print(json_string)\r\n","\r\n","        # Create the BlobServiceClient object and upload the documents\r\n","        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\r\n","        blob_name = blob.name + \"_\"  +str(count)+\".json\"\r\n","        upload_client = upload_blob_data (blob_service_client, destination_container_name, blob_name )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"42fbdd7e-f994-4265-a376-6d1c40652314"}],"metadata":{"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}