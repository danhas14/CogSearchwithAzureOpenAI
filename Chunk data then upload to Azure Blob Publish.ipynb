{"cells":[{"cell_type":"markdown","source":["**This script will take a source directory in Azure blob storage and then download and chunk up the documents in it based on the chunking parameters you provide. It will then create embeddings for the chunks, then upload a resulting JSON file for each chunk that contains the chunked content and associated embeddings back to a destination directory in Azure Blob storage. It will also create an Azure Cognitive Search index, indexer, and data source you can use to easily pull in the data to Azure Cognitive Search. Once the entire script has run you should have the chunked data loaded into the new Azure Cognitive Search index. Make sure to have Azure Cognitive Search Semantic Search turned on. Also make sure to give the Forms Recognizer service managed identity Storage Blob Data Reader access to the blob store.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"570e624f-baa2-42b9-997a-92203b00f123"},{"cell_type":"code","source":["pip install pypdf azure-ai-formrecognizer azure-identity langchain azure-storage-blob python-dotenv unstructured openai azure-search-documents tiktoken"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"719835d5-a617-4618-9553-62af3b1c1df9"},{"cell_type":"code","source":["pip install azure-storage-file-datalake --pre"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a4d7e219-1762-49ce-83af-f45d96b1fa5d"},{"cell_type":"markdown","source":["**Update the fields below with your Azure services information and keys:**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"203a1089-b272-46f3-8148-e4614358cccd"},{"cell_type":"code","source":["import os, uuid\n","from azure.identity import DefaultAzureCredential\n","from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n","from azure.core.credentials import AzureKeyCredential\n","from azure.ai.formrecognizer import DocumentAnalysisClient\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import DefaultAzureCredential\n","from azure.identity import ClientSecretCredential\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import openai\n","import json\n","import requests\n","import random\n","import string\n","import time\n","\n","#OpenAI connectivity \n","openai.api_type = \"azure\"\n","openai.api_key = \"\"\n","openai.api_base = \"https://<your instance>.openai.azure.com/\"\n","openai.api_version = \"2022-12-01\"\n","openai_embeddings_model = \"embeddings-ada-002\" #name of your Ada embeddings deployment, update with correct name\n","\n","#blob access variables\n","\n","connect_str = \"DefaultEndpointsProtocol=https;AccountName=;AccountKey=\" #Get from your Azure storage config\n","blob_account_url = \"https://<your storage account>.blob.core.windows.net\" \n","source_container_name = \"\" #Location of documents to use for the source\n","source_path_name = \"/\"\n","\n","# Create a unique name for the container for the chunked documents - make sure the container exists in Azure storage\n","destination_container_name = \"\"\n","\n","\n","#Name of search index, indexer, and data source you want to create in Azure Cog Search. \n","index_name = \"\"\n","indexer_name = \"\"\n","data_source_name = \"\"\n","\n","#Cognitive Search Connection\n","search_endpoint = \"https://<your cognitive search>.search.windows.net\"\n","search_endpoint_for_creating_index = \"https://<your cognitive search>.search.windows.net/indexes?api-version=2023-07-01-Preview\"\n","search_endpoint_for_creating_indexer = \"https://<your cognitive search>.search.windows.net/indexers?api-version=2023-07-01-Preview\"\n","search_endpoint_for_creating_datasource = \"https://<your cognitive search>.search.windows.net/datasources?api-version=2023-07-01-Preview\"\n","search_api_key =\"\" #Cog search admin key\n","\n","#Forms recognizer details\n","forms_recognizer_url = \"https://<your form recognizer>.cognitiveservices.azure.com\"\n","forms_recognizer_preview_url = \"https://<your form recognizer>.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-read:analyze?api-version=2022-06-30-preview\"\n","forms_recognizer_key = \"\"\n","\n","#Chunk parameters in tokens\n","chunk_size = 1000\n","chunk_overlap = 200"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"f46af4ee-91d4-4dd3-bedc-675b27da77a9","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-08-03T21:11:15.7036539Z","session_start_time":"2023-08-03T21:11:15.8879244Z","execution_start_time":"2023-08-03T21:12:51.6598669Z","execution_finish_time":"2023-08-03T21:12:54.8736386Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"38e17e0a-3215-44a4-ac96-b9fb8c3f8a96"},"text/plain":"StatementMeta(, f46af4ee-91d4-4dd3-bedc-675b27da77a9, 5, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d86dc257-5f53-464d-ad2f-a48a9c1be68d"},{"cell_type":"markdown","source":["**This section will create the index in Azure Cog Search that will store the data after the indexer is run.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9f67df35-3d72-435b-813b-a2252bc0c185"},{"cell_type":"code","source":["## This creates the JSON that will create the search index with vector search enabled.\n","\n","search_json = {\n","    \"name\": f\"{index_name}\",\n","    \"fields\": [\n","        {\n","            \"name\": \"key\",\n","            \"type\": \"Edm.String\",\n","            \"searchable\": False,\n","            \"retrievable\": True,\n","            \"key\": True,\n","            \"filterable\": False,\n","            \"facetable\": False,\n","            \"sortable\": False\n","        },\n","        {\n","            \"name\": \"title\",\n","            \"type\": \"Edm.String\",\n","            \"searchable\": True,\n","            \"retrievable\": True,\n","            \"key\": False,\n","            \"filterable\": False,\n","            \"facetable\": False,\n","            \"sortable\": False\n","        },\n","        {\n","            \"name\": \"content\",\n","            \"type\": \"Edm.String\",\n","            \"searchable\": True,\n","            \"retrievable\": True,\n","            \"key\": False,\n","            \"filterable\": False,\n","            \"facetable\": False,\n","            \"sortable\": False\n","        },\n","        {\n","            \"name\": \"path\",\n","            \"type\": \"Edm.String\",\n","            \"searchable\": True,\n","            \"retrievable\": True,\n","            \"key\": False,\n","            \"filterable\": False,\n","            \"facetable\": False,\n","            \"sortable\": False\n","        },        \n","        {\n","            \"name\": \"titleVector\",\n","            \"type\": \"Collection(Edm.Single)\",\n","            \"searchable\": True,\n","            \"retrievable\": False,\n","            \"dimensions\": 1536,\n","            \"vectorSearchConfiguration\": \"my-vector-config\"\n","        },\n","        {\n","            \"name\": \"contentVector\",\n","            \"type\": \"Collection(Edm.Single)\",\n","            \"searchable\": True,\n","            \"retrievable\": False,\n","            \"dimensions\": 1536,\n","            \"vectorSearchConfiguration\": \"my-vector-config\"\n","        }\n","    ],\n","    \"corsOptions\": {\n","        \"allowedOrigins\": [\n","            \"*\"\n","        ],\n","        \"maxAgeInSeconds\": 60\n","    },\n","    \"vectorSearch\": {\n","        \"algorithmConfigurations\": [\n","            {\n","                \"name\": \"my-vector-config\",\n","                \"kind\": \"hnsw\",\n","                \"hnswParameters\": {\n","                    \"m\": 4,\n","                    \"efConstruction\": 400,\n","                    \"metric\": \"cosine\"\n","                }\n","            }\n","        ]\n","    },\n","    \"semantic\": {\n","        \"configurations\": [\n","            {\n","                \"name\": \"my-semantic-config\",\n","                \"prioritizedFields\": {\n","                    \"titleField\": {\n","                        \"fieldName\": \"title\"\n","                    },\n","                    \"prioritizedContentFields\": [\n","                        {\n","                            \"fieldName\": \"content\"\n","                        }\n","                    ],\n","                    \"prioritizedKeywordsFields\": [\n","                        {\n","                            \"fieldName\": \"content\"\n","                        }\n","                    ]\n","                }\n","            }\n","        ]\n","    }\n","}\n","\n","#This section makes the REST call to create the index\n","\n","url = f'{search_endpoint_for_creating_index }'\n","print(\"URL is \" + url)\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\n","print(\"Search api key is\" +search_api_key)\n","print(\"Search JSON is \" + str(search_json))\n","print(\"About to make the rest call\")\n","#print(\"data is\" + str(search_json))\n","response = requests.post(search_endpoint_for_creating_index , headers=headers, data=json.dumps(search_json))\n","\n","if response.status_code == 200 or response.status_code == 201:\n","    data = response.json()\n","    print(data)\n","else:\n","    print('Error:', response.status_code)       \n","\n","print(\"rest call completed\")   "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b79fece1-c597-41cd-914f-b68a84a9f497"},{"cell_type":"markdown","source":["**This section will create the data source in Azure Cog Search that the indexer will use to load the data. It references the location of the chunked data above**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3aebb233-892c-49a4-8b2a-1d1c70c75c99"},{"cell_type":"code","source":["data_source_request_json = {   \n","    \"name\" : data_source_name,    \n","    \"type\" : \"azureblob\",\n","    \"credentials\" : { \"connectionString\" :connect_str },\n","    \"container\": {\n","        \"name\": destination_container_name\n","    }\n","}  \n","\n","url = f'{search_endpoint_for_creating_index }'\n","print(\"URL is \" + url)\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\n","#print(\"Search api key is\" +search_api_key)\n","print(\"About to make the REST call\")\n","print(\"Request JSON is \" +json.dumps(request_json))\n","#print(\"data is\" + str(search_json))\n","response = requests.post(search_endpoint_for_creating_datasource , headers=headers, data=json.dumps(data_source_request_json))\n","\n","data = response.json()\n","print(data)\n","\n","if response.status_code == 200 or response.status_code == 201:\n","    print(\"Create data source call completed successfully\") \n","else:\n","    print('Error:', response.status_code)       \n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fdf21b54-f126-4e8d-b390-f40c6fa88225"},{"cell_type":"markdown","source":["**This section below will read the data from the source storage account, chunk it, create embeddings, then upload a JSON file that can be ingested into Azure Cognitive Search when the indexer is run. **\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd59faa4-3dcb-45c4-a976-77aa49d59459"},{"cell_type":"code","source":["\n","# This function below is for loading the chunks back to Azure storage, called at the end of this section\n","def upload_blob_data(blob_service_client: BlobServiceClient, destination_container_name: str, blob_name):\n","    blob_client = blob_service_client.get_blob_client(container=destination_container_name, blob=blob_name)\n","    data = json_string\n","    # Upload the blob data - default blob type is BlockBlob\n","    blob_client.upload_blob(data, blob_type=\"BlockBlob\")\n","\n","def list_blobs_flat(blob_service_client: BlobServiceClient, source_container_name):\n","    container_client = blob_service_client.get_container_client(container=source_container_name)\n","    blob_list = container_client.list_blobs()\n","    #for blob in blob_list:\n","    #    print(f\"{blob.name}\")\n","    return blob_list\n","\n","#This function generates a random string we can use for the Azure Search key field\n","def generate_random_string(length):\n","    characters = string.ascii_letters + string.digits\n","    random_string = ''.join(random.choice(characters) for _ in range(length))\n","    return random_string\n","\n","\n","# Function to generate embeddings for content\n","def generate_embeddings(text):\n","    response = openai.Embedding.create(\n","    input=text, engine=f\"{openai_embeddings_model}\") #engine = deployment name of your ada-0002 model\n","    embeddings = response['data'][0]['embedding']\n","    return embeddings\n","\n","blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n","blob_listing = list_blobs_flat(blob_service_client, source_container_name)\n","#The section below will loop through each of the documents paths, parse the files using forms recognizer, generate the embeddings, then upload each chunk to Azure storage\n","count = 0\n","for blob in blob_listing:\n","    print(blob.name + '\\n')\n","\n","    ##This section uses Forms Recognizer to read a file in blob store. \n","    endpoint = forms_recognizer_url\n","    key = forms_recognizer_key\n","    formUrl =  f\"{blob_account_url}/{source_container_name}/{blob.name}\"\n","    print(formUrl)\n","\n","    document_analysis_client = DocumentAnalysisClient(\n","        endpoint=endpoint, credential=AzureKeyCredential(key)\n","        )\n","\n","    poller = document_analysis_client.begin_analyze_document_from_url(\n","        \"prebuilt-read\", formUrl\n","    )\n","    result = poller.result()\n","\n","    #print(\"Document contains content: \", result.content)\n","    content = result.content\n","\n","    #This will chunk up the document\n","    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n","    )\n","    texts = text_splitter.split_text(content)\n","\n","\n","    #print(texts[0])\n","\n","    x = len(texts)\n","    #print(x)\n","\n","    #This takes the url of the document and stores it with no spaces\n","    formUrl = formUrl.replace(\" \", \"%20\")\n","    print(\"Looping through each of the chunks and writing them back to blob storage in a JSON format\")\n","    for item in texts:\n","        count = count + 1\n","        ##print(item)\n","        item = str(item)\n","        content_embeddings = generate_embeddings(item)\n","        time.sleep(10) #To stay under quota\n","        title_embeddings = generate_embeddings(blob.name)\n","\n","        # Generate a random string of length 15 for the Azure Search key\n","        random_str = generate_random_string(15)\n","\n","        #item = str(item) //use this for just straight chunking without using a json format\n","        json_data = {\n","            \"key\": f\"{random_str}\",\n","            \"title\": f\"{blob.name}\",\n","            \"content\": f\"{item}\",\n","            \"path\": f\"{formUrl}\",\n","            \"titleVector\": f\"{title_embeddings}\",\n","            \"contentVector\": f\"{content_embeddings}\"\n","        }\n","\n","        json_string = json.dumps(json_data)\n","        #print(json_string)\n","\n","        # Create the BlobServiceClient object and upload the documents\n","        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n","        blob_name = blob.name + \"_\"  +str(count)+\".json\"\n","        upload_client = upload_blob_data (blob_service_client, destination_container_name, blob_name )"],"outputs":[],"execution_count":null,"metadata":{},"id":"edd0e598-16b7-48fa-8b87-e14bee4ad8be"},{"cell_type":"markdown","source":["**This section will create the Azure Cognitive Search indexer. It will automatically run and load the data to the index.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8ac89bae-8c45-45e6-809a-3f221770a676"},{"cell_type":"code","source":["# This code creates an indexer that will load the data to the index above\n","request_json = {\n","    \"name\": indexer_name,\n","    \"dataSourceName\": data_source_name,\n","    \"targetIndexName\": index_name,\n","    \"parameters\": {\n","        \"configuration\": {\n","        \"allowSkillsetToReadFileData\": False,\n","        \"parsingMode\": \"json\"\n","        }\n","    },\n","    \"fieldMappings\": [],\n","    \"outputFieldMappings\": [],\n","    }\n","\n","\n","url = f'{search_endpoint_for_creating_index }'\n","print(\"URL is \" + url)\n","headers = {'Content-Type': 'application/json', 'api-key': search_api_key}\n","print(\"Search api key is\" +search_api_key)\n","print(\"About to make the rest call\")\n","print(\"request json is \" +json.dumps(request_json))\n","#print(\"data is\" + str(search_json))\n","response = requests.post(search_endpoint_for_creating_indexer , headers=headers, data=json.dumps(request_json))\n","\n","data = response.json()\n","print(data)\n","\n","if response.status_code == 200 or response.status_code == 201:\n","    print(\"Create indexer call completed successfully\") \n","else:\n","    print('Error:', response.status_code)       \n","\n","print(\"Indexer call completed\") "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"494e6a36-ebe6-4f75-8677-beba67bcccf7"}],"metadata":{"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}